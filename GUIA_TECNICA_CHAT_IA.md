# üß† Gu√≠a T√©cnica Detallada: Sistema de Chat con IA Veterinario

## üìã Tabla de Contenidos

1. [Arquitectura del Sistema](#arquitectura-del-sistema)
2. [Flujo de Datos Completo](#flujo-de-datos-completo)
3. [Componentes Principales](#componentes-principales)
4. [Tecnolog√≠as Utilizadas](#tecnolog√≠as-utilizadas)
5. [Alternativas Tecnol√≥gicas](#alternativas-tecnol√≥gicas)
6. [Enfoques de Implementaci√≥n](#enfoques-de-implementaci√≥n)
7. [Memoria Conversacional](#memoria-conversacional)
8. [RAG (Retrieval Augmented Generation)](#rag-retrieval-augmented-generation)
9. [Optimizaciones y Mejores Pr√°cticas](#optimizaciones-y-mejores-pr√°cticas)

---

## üèóÔ∏è Arquitectura del Sistema

### Diagrama de Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CLIENTE (Frontend)                       ‚îÇ
‚îÇ  - React/Next.js                                                ‚îÇ
‚îÇ  - Hace peticiones HTTP a la API                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ HTTP Request
                              ‚îÇ POST /chat/pets/{pet_id}/ask
                              ‚îÇ Headers: Authorization: Bearer <token>
                              ‚îÇ Body: { question, session_id }
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASTAPI (app/routes/chat.py)                  ‚îÇ
‚îÇ  - Valida autenticaci√≥n                                          ‚îÇ
‚îÇ  - Valida esquemas Pydantic                                      ‚îÇ
‚îÇ  - Maneja errores HTTP                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ Llama a ChatController
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              CONTROLADOR (app/controllers/chat.py)               ‚îÇ
‚îÇ  - Gestiona sesiones de conversaci√≥n                             ‚îÇ
‚îÇ  - Maneja memoria conversacional                                 ‚îÇ
‚îÇ  - Limita memoria a 6 interacciones (12 mensajes)               ‚îÇ
‚îÇ  - Obtiene documentos de la mascota                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ Crea/Usa LangChainService
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          SERVICIO LANGCHAIN (app/services/langchain_service.py)  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 1. Carga Documentos PDF desde S3                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - PyPDFLoader descarga y procesa PDFs                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚îÇ                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 2. Divide en Chunks                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - RecursiveCharacterTextSplitter                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - Chunk size: 1000 caracteres                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - Overlap: 200 caracteres                             ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚îÇ                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 3. Genera Embeddings                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - OpenAIEmbeddings (text-embedding-3-small)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - Convierte texto a vectores de 1536 dimensiones     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚îÇ                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 4. Almacena en Vector Store                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - PGVector (PostgreSQL + pgvector extension)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - B√∫squeda por similitud sem√°ntica                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚îÇ                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 5. Crea Cadena Conversacional                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - ConversationalRetrievalChain (con documentos)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - SimpleConversationChain (sin documentos)             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - ConversationBufferMemory para historial              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚îÇ                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 6. Invoca LLM                                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - ChatOpenAI (gpt-4o-mini)                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - Recibe: pregunta + historial + documentos          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    - Retorna: respuesta generada                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ Respuesta
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    RESPUESTA AL CLIENTE                          ‚îÇ
‚îÇ  {                                                               ‚îÇ
‚îÇ    "answer": "Respuesta del veterinario...",                    ‚îÇ
‚îÇ    "source_documents": [...],                                    ‚îÇ
‚îÇ    "chat_history": [...],                                        ‚îÇ
‚îÇ    "session_id": "...",                                          ‚îÇ
‚îÇ    "memory_info": {...}                                          ‚îÇ
‚îÇ  }                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Flujo de Datos Completo

### Paso 1: Cliente hace petici√≥n

```http
POST /chat/pets/876835fa-6c7d-4c97-bc18-4e5728e8bc13/ask
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
Content-Type: application/json

{
  "question": "¬øPuedes leer el documento de vacunaci√≥n de mi mascota?",
  "session_id": "user123_pet456"
}
```

### Paso 2: FastAPI valida y enruta

**Archivo:** `app/routes/chat.py`

```python
@router.post("/pets/{pet_id}/ask")
async def ask_veterinary_question(
    pet_id: str,
    request: ChatQuestionRequest,
    current_user: User = Depends(get_current_active_user),
    db: Session = Depends(get_db)
):
    # 1. Valida autenticaci√≥n (get_current_active_user)
    # 2. Valida esquema (ChatQuestionRequest)
    # 3. Llama al controlador
    result = ChatController.ask_question_about_pet(...)
    return ChatResponse(**result)
```

**Validaciones:**
- ‚úÖ Token JWT v√°lido
- ‚úÖ Usuario activo
- ‚úÖ Esquema Pydantic v√°lido
- ‚úÖ Manejo de errores HTTP

### Paso 3: Controlador gestiona la l√≥gica

**Archivo:** `app/controllers/chat.py`

```python
def ask_question_about_pet(...):
    # 1. Verifica que la mascota existe y pertenece al usuario
    pet = ChatController.get_pet_by_id(db, pet_id, current_user)
    
    # 2. Inicializa servicio LangChain
    langchain_service = LangChainService()
    
    # 3. Obtiene documentos PDF de la mascota desde la BD
    pdf_urls = langchain_service.get_pet_documents_from_db(db, pet_id)
    
    # 4. Crea vector store si hay documentos
    if pdf_urls:
        vector_store = langchain_service.create_vector_store(pdf_urls, pet_id)
        use_documents = True
    else:
        vector_store = None
        use_documents = False
    
    # 5. Obtiene o crea memoria conversacional
    if session_id not in ChatController._conversation_memories:
        memory = ConversationBufferMemory(...)
        ChatController._conversation_memories[session_id] = memory
    else:
        memory = ChatController._conversation_memories[session_id]
    
    # 6. Limita memoria a 6 interacciones (12 mensajes)
    ChatController._limit_memory_messages(memory)
    
    # 7. Hace la pregunta usando LangChain
    result = langchain_service.ask_question(
        question=question,
        vector_store=vector_store,
        memory=memory,
        use_documents=use_documents
    )
    
    # 8. Retorna respuesta formateada
    return {
        "answer": result["answer"],
        "source_documents": result["source_documents"],
        "chat_history": result["chat_history"],
        "session_id": session_id,
        "memory_info": {...}
    }
```

### Paso 4: Servicio LangChain procesa

**Archivo:** `app/services/langchain_service.py`

#### 4.1. Si hay documentos (RAG):

```python
def ask_question(question, vector_store, memory, use_documents=True):
    # 1. Crea cadena conversacional con RAG
    chain = ConversationalRetrievalChain.from_llm(
        llm=self.llm,                    # ChatOpenAI
        retriever=vector_store.as_retriever(k=4),  # Top 4 documentos
        memory=memory,                     # ConversationBufferMemory
        return_source_documents=True
    )
    
    # 2. Invoca la cadena
    result = chain.invoke({"question": question})
    
    # 3. Extrae respuesta y documentos
    answer = result["answer"]
    source_docs = result["source_documents"]
    
    return {"answer": answer, "source_documents": source_docs, ...}
```

**Flujo interno de ConversationalRetrievalChain:**

```
1. Recibe pregunta: "¬øCu√°ndo fue la √∫ltima vacunaci√≥n?"
2. Obtiene historial de memoria: [mensajes anteriores]
3. Busca documentos relevantes en vector store:
   - Convierte pregunta a embedding
   - Busca top 4 chunks m√°s similares
   - Recupera: "Vacunaci√≥n aplicada el 17/01/2019..."
4. Construye prompt con:
   - System prompt (instrucciones de veterinario)
   - Historial de conversaci√≥n
   - Documentos relevantes
   - Pregunta actual
5. Env√≠a a OpenAI GPT-4o-mini
6. Recibe respuesta generada
7. Guarda pregunta y respuesta en memoria
8. Retorna respuesta + documentos fuente
```

#### 4.2. Si NO hay documentos (conversaci√≥n general):

```python
def ask_question(question, vector_store=None, memory, use_documents=False):
    # 1. Crea cadena conversacional simple
    class SimpleConversationChain:
        def invoke(self, inputs):
            # Obtiene historial de memoria
            memory_vars = memory.load_memory_variables({})
            history = memory_vars.get('chat_history', [])
            
            # Construye mensajes
            messages = [
                SystemMessage(content=VETERINARY_SYSTEM_PROMPT),
                *history,  # Historial previo
                HumanMessage(content=inputs["input"])  # Pregunta actual
            ]
            
            # Invoca LLM directamente
            response = self.llm.invoke(messages)
            answer = response.content
            
            # Guarda en memoria
            memory.save_context(
                {"input": inputs["input"]},
                {"output": answer}
            )
            
            return {"response": answer}
    
    chain = SimpleConversationChain(...)
    result = chain.invoke({"input": question})
    return {"answer": result["response"], ...}
```

### Paso 5: Procesamiento de Documentos (RAG)

Cuando hay documentos PDF, el sistema:

#### 5.1. Descarga PDFs desde S3

```python
def _download_pdf_from_s3(s3_url: str) -> str:
    # 1. Crea archivo temporal
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
    
    # 2. Descarga desde S3 usando requests
    response = requests.get(s3_url, stream=True)
    
    # 3. Guarda en archivo temporal
    with open(temp_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    return temp_path
```

#### 5.2. Carga y procesa PDFs

```python
def _load_pdf_documents(pdf_urls: List[str]) -> List[Document]:
    all_documents = []
    
    for pdf_url in pdf_urls:
        # Descarga PDF
        temp_path = self._download_pdf_from_s3(pdf_url)
        
        # Carga con PyPDFLoader
        loader = PyPDFLoader(temp_path)
        documents = loader.load()  # Lista de Document (una por p√°gina)
        
        # Agrega metadata
        for doc in documents:
            doc.metadata['source'] = pdf_url
            doc.metadata['source_type'] = 'pet_document'
        
        all_documents.extend(documents)
        
        # Limpia archivo temporal
        os.unlink(temp_path)
    
    return all_documents
```

#### 5.3. Divide en Chunks

```python
# RecursiveCharacterTextSplitter divide el texto en fragmentos
chunks = self.text_splitter.split_documents(documents)

# Ejemplo:
# Documento original: 5000 caracteres
# Chunks creados:
#   - Chunk 1: caracteres 0-1000
#   - Chunk 2: caracteres 800-1800  (200 de overlap)
#   - Chunk 3: caracteres 1600-2600
#   - ...
```

**¬øPor qu√© dividir en chunks?**
- Los LLMs tienen l√≠mite de tokens por contexto
- Permite buscar solo las partes relevantes
- Mejora la precisi√≥n de la b√∫squeda sem√°ntica

#### 5.4. Genera Embeddings

```python
# OpenAIEmbeddings convierte texto a vectores
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Para cada chunk:
vector = embeddings.embed_query("Vacunaci√≥n aplicada el 17/01/2019...")
# Resultado: array de 1536 n√∫meros (dimensiones)

# Ejemplo de vector (simplificado):
# [0.123, -0.456, 0.789, ..., 0.234]  # 1536 n√∫meros
```

**¬øQu√© son los embeddings?**
- Representaci√≥n num√©rica del significado del texto
- Textos similares tienen vectores similares
- Permite b√∫squeda sem√°ntica (no solo palabras clave)

#### 5.5. Almacena en Vector Store

```python
# PGVector almacena vectores en PostgreSQL
vector_store = PGVector.from_documents(
    documents=chunks,
    embedding=self.embeddings,
    collection_name=f"pet_{pet_id}_documents",
    connection_string=connection_string
)

# Estructura en PostgreSQL:
# Tabla: langchain_pg_embedding
# Columnas:
#   - uuid: ID del chunk
#   - collection_id: ID de la colecci√≥n
#   - embedding: vector(1536)  # Vector de 1536 dimensiones
#   - document: JSON con metadata y contenido
#   - cmetadata: Metadata adicional
```

**B√∫squeda por similitud:**
```sql
-- Cuando se busca "vacunaci√≥n"
SELECT document, embedding
FROM langchain_pg_embedding
WHERE collection_id = 'pet_123_documents'
ORDER BY embedding <-> '[vector de la pregunta]'  -- Distancia coseno
LIMIT 4;
```

### Paso 6: Generaci√≥n de Respuesta

#### 6.1. Con RAG (Retrieval Augmented Generation)

```python
# Prompt construido autom√°ticamente por ConversationalRetrievalChain
prompt = f"""
Eres un veterinario experto...

Historial de conversaci√≥n:
{chat_history}

Documentos relevantes:
{context}  # Top 4 chunks m√°s similares

Pregunta: {question}

Respuesta:
"""

# Se env√≠a a OpenAI
response = llm.invoke(prompt)
```

#### 6.2. Sin documentos (conversaci√≥n general)

```python
# Mensajes estructurados
messages = [
    SystemMessage(content="Eres un veterinario experto..."),
    HumanMessage(content="Pregunta anterior 1"),
    AIMessage(content="Respuesta anterior 1"),
    HumanMessage(content="Pregunta actual")
]

# Se env√≠a a OpenAI Chat API
response = llm.invoke(messages)
```

### Paso 7: Actualizaci√≥n de Memoria

```python
# Despu√©s de recibir respuesta
memory.save_context(
    {"input": question},
    {"output": answer}
)

# La memoria ahora contiene:
# chat_history = [
#     HumanMessage(content="Pregunta 1"),
#     AIMessage(content="Respuesta 1"),
#     HumanMessage(content="Pregunta 2"),
#     AIMessage(content="Respuesta 2"),
#     HumanMessage(content="Pregunta actual"),
#     AIMessage(content="Respuesta actual")
# ]
```

### Paso 8: Limitaci√≥n de Memoria

```python
def _limit_memory_messages(memory):
    # Obtiene todos los mensajes
    memory_vars = memory.load_memory_variables({})
    messages = memory_vars.get('chat_history', [])
    
    # Si excede 12 mensajes (6 interacciones)
    if len(messages) > 12:
        # Mantiene solo los √∫ltimos 12
        messages_to_keep = messages[-12:]
        
        # Limpia y restaura
        memory.chat_memory.clear()
        for msg in messages_to_keep:
            memory.chat_memory.add_message(msg)
```

---

## üß© Componentes Principales

### 1. Endpoints (app/routes/chat.py)

#### `POST /chat/pets/{pet_id}/ask`
- **Prop√≥sito:** Hacer preguntas al veterinario IA
- **Autenticaci√≥n:** Requerida (JWT)
- **Input:** `{ question, session_id? }`
- **Output:** `{ answer, source_documents, chat_history, session_id, memory_info }`

#### `GET /chat/sessions/{session_id}/history`
- **Prop√≥sito:** Obtener historial completo de una conversaci√≥n
- **Autenticaci√≥n:** Requerida
- **Output:** `{ session_id, history, message_count }`

#### `DELETE /chat/sessions/{session_id}`
- **Prop√≥sito:** Limpiar memoria de una conversi√≥n
- **Autenticaci√≥n:** Requerida
- **Output:** `{ message, session_id, status }`

#### `GET /chat/sessions`
- **Prop√≥sito:** Listar todas las sesiones activas
- **Autenticaci√≥n:** Requerida
- **Output:** `{ active_sessions, total_count }`

#### `GET /chat/sessions/{session_id}/stats`
- **Prop√≥sito:** Obtener estad√≠sticas de uso de memoria
- **Autenticaci√≥n:** Requerida
- **Output:** `{ session_id, message_count, max_messages, interactions_count, ... }`

---

### 2. Controlador (app/controllers/chat.py)

**Responsabilidades:**
- ‚úÖ Validar que la mascota existe y pertenece al usuario
- ‚úÖ Gestionar sesiones de conversaci√≥n (crear, obtener, limpiar)
- ‚úÖ Gestionar memoria conversacional (limitar a 6 interacciones)
- ‚úÖ Coordinar entre rutas y servicios
- ‚úÖ Formatear respuestas

**Almacenamiento de Memoria:**
```python
# En memoria (actual - no persistente)
_conversation_memories: Dict[str, ConversationBufferMemory] = {}

# En producci√≥n deber√≠a usar:
# - Redis (recomendado)
# - PostgreSQL (tabla de conversaciones)
# - MongoDB (colecci√≥n de mensajes)
```

---

### 3. Servicio LangChain (app/services/langchain_service.py)

**Responsabilidades:**
- ‚úÖ Cargar y procesar documentos PDF
- ‚úÖ Crear y gestionar vector stores
- ‚úÖ Generar embeddings
- ‚úÖ Crear cadenas conversacionales (con y sin RAG)
- ‚úÖ Invocar LLM
- ‚úÖ Extraer historial de conversaci√≥n

**Clases y M√©todos Principales:**

```python
class LangChainService:
    def __init__(self):
        # Inicializa embeddings y LLM
        self.embeddings = OpenAIEmbeddings(...)
        self.llm = ChatOpenAI(...)
        self.text_splitter = RecursiveCharacterTextSplitter(...)
    
    def get_pet_documents_from_db(self, db, pet_id):
        # Obtiene URLs de PDFs desde PostgreSQL
        # Retorna: List[str] de URLs S3
    
    def _download_pdf_from_s3(self, s3_url):
        # Descarga PDF desde S3 a archivo temporal
        # Retorna: str (ruta al archivo temporal)
    
    def _load_pdf_documents(self, pdf_urls):
        # Carga PDFs usando PyPDFLoader
        # Retorna: List[Document]
    
    def create_vector_store(self, pdf_urls, pet_id):
        # 1. Carga documentos
        # 2. Divide en chunks
        # 3. Genera embeddings
        # 4. Almacena en PGVector
        # Retorna: PGVector
    
    def ask_question(self, question, vector_store, memory, use_documents):
        # Decide si usar RAG o conversaci√≥n general
        # Invoca la cadena apropiada
        # Retorna: Dict con respuesta y metadata
    
    def _ask_with_rag(self, question, vector_store, memory):
        # Usa ConversationalRetrievalChain
        # Busca documentos relevantes
        # Genera respuesta con contexto
    
    def _ask_without_documents(self, question, memory):
        # Usa SimpleConversationChain
        # Solo usa historial conversacional
        # Genera respuesta general
```

---

## üîß Tecnolog√≠as Utilizadas

### 1. LangChain

**¬øQu√© es?**
- Framework de Python para construir aplicaciones con LLMs
- Proporciona abstracciones para chains, memory, prompts, etc.

**Componentes usados:**
- `ConversationalRetrievalChain`: Cadena que combina RAG con memoria
- `ConversationBufferMemory`: Almacena historial de conversaci√≥n
- `PGVector`: Vector store basado en PostgreSQL
- `RecursiveCharacterTextSplitter`: Divide documentos en chunks
- `PyPDFLoader`: Carga documentos PDF

**Ventajas:**
- ‚úÖ Abstracciones de alto nivel
- ‚úÖ Integraci√≥n f√°cil con m√∫ltiples LLMs
- ‚úÖ Manejo autom√°tico de memoria
- ‚úÖ Ecosistema grande y activo

**Desventajas:**
- ‚ùå Puede ser lento (muchas capas de abstracci√≥n)
- ‚ùå Cambios frecuentes en la API
- ‚ùå Curva de aprendizaje

---

### 2. OpenAI

**Modelos usados:**
- **LLM:** `gpt-4o-mini` (ChatOpenAI)
  - Prop√≥sito: Generar respuestas
  - Ventajas: R√°pido, econ√≥mico, buena calidad
  - Alternativas: `gpt-4o`, `gpt-4-turbo`, `gpt-3.5-turbo`

- **Embeddings:** `text-embedding-3-small`
  - Prop√≥sito: Convertir texto a vectores
  - Dimensiones: 1536
  - Alternativas: `text-embedding-3-large` (3072 dimensiones, m√°s preciso)

**API de OpenAI:**
```python
# Chat Completions API
POST https://api.openai.com/v1/chat/completions
{
  "model": "gpt-4o-mini",
  "messages": [
    {"role": "system", "content": "Eres un veterinario..."},
    {"role": "user", "content": "Pregunta"}
  ],
  "temperature": 0.3
}

# Embeddings API
POST https://api.openai.com/v1/embeddings
{
  "model": "text-embedding-3-small",
  "input": "Texto a convertir"
}
```

---

### 3. PostgreSQL + pgvector

**pgvector:**
- Extensi√≥n de PostgreSQL para almacenar vectores
- Permite b√∫squeda por similitud (cosine distance, L2 distance)

**Estructura de datos:**
```sql
CREATE TABLE langchain_pg_embedding (
    uuid UUID PRIMARY KEY,
    collection_id UUID,
    embedding vector(1536),  -- Vector de 1536 dimensiones
    document JSONB,           -- Contenido del chunk
    cmetadata JSONB          -- Metadata adicional
);

-- √çndice para b√∫squeda r√°pida
CREATE INDEX ON langchain_pg_embedding 
USING ivfflat (embedding vector_cosine_ops);
```

**B√∫squeda:**
```sql
-- Encuentra chunks m√°s similares
SELECT document, embedding <-> '[vector de pregunta]' as distance
FROM langchain_pg_embedding
WHERE collection_id = 'pet_123_documents'
ORDER BY distance
LIMIT 4;
```

---

### 4. AWS S3

**Prop√≥sito:**
- Almacenar documentos PDF de mascotas
- URLs p√∫blicas para descargar PDFs

**Flujo:**
```
1. Usuario sube PDF ‚Üí FastAPI
2. FastAPI ‚Üí S3Service.upload_document()
3. S3Service ‚Üí AWS S3 (put_object)
4. Retorna URL p√∫blica
5. URL se guarda en PostgreSQL (pet_photos.url)
6. LangChain descarga desde URL cuando se necesita
```

---

## üîÑ Alternativas Tecnol√≥gicas

### 1. Alternativas a LangChain

#### Opci√≥n A: LlamaIndex

**¬øQu√© es?**
- Framework especializado en RAG y aplicaciones de datos
- M√°s enfocado en indexaci√≥n y recuperaci√≥n

**Ventajas:**
- ‚úÖ Mejor rendimiento en RAG
- ‚úÖ M√°s opciones de √≠ndices (vector, keyword, hybrid)
- ‚úÖ Mejor manejo de documentos grandes
- ‚úÖ Integraci√≥n con m√°s bases de datos vectoriales

**Desventajas:**
- ‚ùå Menos opciones para memoria conversacional
- ‚ùå Ecosistema m√°s peque√±o

**Ejemplo de implementaci√≥n:**
```python
from llama_index import VectorStoreIndex, Document
from llama_index.vector_stores import PGVectorStore
from llama_index.llms import OpenAI

# Crear √≠ndice
vector_store = PGVectorStore(...)
index = VectorStoreIndex.from_documents(documents, vector_store=vector_store)

# Hacer pregunta
query_engine = index.as_query_engine()
response = query_engine.query("¬øCu√°ndo fue la √∫ltima vacunaci√≥n?")
```

---

#### Opci√≥n B: Directo con OpenAI + SQL

**Enfoque:**
- Usar OpenAI API directamente sin frameworks
- Implementar RAG manualmente
- Usar SQL para b√∫squeda de documentos

**Ventajas:**
- ‚úÖ Control total sobre el flujo
- ‚úÖ Menos dependencias
- ‚úÖ M√°s r√°pido (menos capas)
- ‚úÖ M√°s f√°cil de debuggear

**Desventajas:**
- ‚ùå M√°s c√≥digo para escribir
- ‚ùå Tienes que implementar todo manualmente
- ‚ùå Sin abstracciones √∫tiles

**Ejemplo de implementaci√≥n:**
```python
import openai
from pgvector.psycopg import register_vector
import psycopg

# 1. Generar embedding de la pregunta
response = openai.embeddings.create(
    model="text-embedding-3-small",
    input=question
)
question_embedding = response.data[0].embedding

# 2. Buscar documentos similares en PostgreSQL
conn = psycopg.connect(DATABASE_URL)
register_vector(conn)

with conn.cursor() as cur:
    cur.execute("""
        SELECT document, embedding <-> %s::vector as distance
        FROM langchain_pg_embedding
        WHERE collection_id = %s
        ORDER BY distance
        LIMIT 4
    """, (question_embedding, collection_id))
    
    results = cur.fetchall()

# 3. Construir prompt con documentos
context = "\n".join([r[0] for r in results])
prompt = f"Documentos:\n{context}\n\nPregunta: {question}"

# 4. Llamar a OpenAI
response = openai.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "Eres un veterinario..."},
        {"role": "user", "content": prompt}
    ]
)

answer = response.choices[0].message.content
```

---

#### Opci√≥n C: Haystack (by deepset)

**¬øQu√© es?**
- Framework de NLP para b√∫squeda y QA
- Muy bueno para RAG y pipelines complejos

**Ventajas:**
- ‚úÖ Excelente para RAG
- ‚úÖ Muchos componentes pre-construidos
- ‚úÖ Buen rendimiento
- ‚úÖ Soporte para m√∫ltiples bases de datos vectoriales

**Desventajas:**
- ‚ùå Curva de aprendizaje m√°s pronunciada
- ‚ùå Menos popular que LangChain

**Ejemplo:**
```python
from haystack import Pipeline
from haystack.components.embedders import OpenAIDocumentEmbedder
from haystack.components.retrievers import InMemoryEmbeddingRetriever
from haystack.components.generators import OpenAIGenerator

# Crear pipeline
pipeline = Pipeline()
pipeline.add_component("embedder", OpenAIDocumentEmbedder(...))
pipeline.add_component("retriever", InMemoryEmbeddingRetriever(...))
pipeline.add_component("generator", OpenAIGenerator(...))

# Conectar componentes
pipeline.connect("embedder", "retriever")
pipeline.connect("retriever", "generator")

# Ejecutar
result = pipeline.run({"embedder": {"documents": documents}})
```

---

### 2. Alternativas a OpenAI

#### Opci√≥n A: Anthropic Claude (Claude API)

**Ventajas:**
- ‚úÖ Mejor contexto (hasta 200K tokens)
- ‚úÖ Muy bueno para an√°lisis de documentos largos
- ‚úÖ Respuestas m√°s estructuradas

**Desventajas:**
- ‚ùå M√°s caro
- ‚ùå API diferente (requiere cambios en c√≥digo)

**Ejemplo:**
```python
from anthropic import Anthropic

client = Anthropic(api_key="...")

response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": question}
    ]
)
```

---

#### Opci√≥n B: Google Gemini

**Ventajas:**
- ‚úÖ Gratis hasta cierto l√≠mite
- ‚úÖ Buen rendimiento
- ‚úÖ Integraci√≥n con Google Cloud

**Desventajas:**
- ‚ùå Menos maduro que OpenAI
- ‚ùå API puede cambiar

**Ejemplo:**
```python
import google.generativeai as genai

genai.configure(api_key="...")
model = genai.GenerativeModel('gemini-pro')

response = model.generate_content(question)
```

---

#### Opci√≥n C: Modelos Locales (Ollama, LlamaIndex)

**Ventajas:**
- ‚úÖ 100% privado (datos no salen del servidor)
- ‚úÖ Sin costos por API
- ‚úÖ Control total

**Desventajas:**
- ‚ùå Requiere hardware potente (GPU)
- ‚ùå Menor calidad que modelos cloud
- ‚ùå M√°s complejo de configurar

**Ejemplo con Ollama:**
```python
from langchain_community.llms import Ollama

llm = Ollama(model="llama2")

response = llm.invoke(question)
```

---

### 3. Alternativas a pgvector

#### Opci√≥n A: Pinecone

**Ventajas:**
- ‚úÖ Servicio gestionado (no necesitas configurar)
- ‚úÖ Muy r√°pido
- ‚úÖ Escalable autom√°ticamente

**Desventajas:**
- ‚ùå Costo adicional
- ‚ùå Dependencia externa
- ‚ùå Datos fuera de tu control

**Ejemplo:**
```python
from pinecone import Pinecone

pc = Pinecone(api_key="...")
index = pc.Index("pet-documents")

# Insertar
index.upsert(vectors=[...])

# Buscar
results = index.query(
    vector=question_embedding,
    top_k=4
)
```

---

#### Opci√≥n B: Weaviate

**Ventajas:**
- ‚úÖ Open source
- ‚úÖ Muy r√°pido
- ‚úÖ Buenas caracter√≠sticas de b√∫squeda

**Desventajas:**
- ‚ùå Requiere servidor separado
- ‚ùå M√°s complejo de configurar

**Ejemplo:**
```python
import weaviate

client = weaviate.Client("http://localhost:8080")

# Buscar
result = client.query.get("PetDocument", ["content"])\
    .with_near_vector({"vector": question_embedding})\
    .with_limit(4)\
    .do()
```

---

#### Opci√≥n C: Chroma

**Ventajas:**
- ‚úÖ Muy f√°cil de usar
- ‚úÖ Puede ser embebido o servidor
- ‚úÖ Buen para desarrollo

**Desventajas:**
- ‚ùå Menos robusto para producci√≥n
- ‚ùå Menos caracter√≠sticas avanzadas

**Ejemplo:**
```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("pet_documents")

# Insertar
collection.add(
    documents=["chunk 1", "chunk 2"],
    embeddings=[...],
    ids=["id1", "id2"]
)

# Buscar
results = collection.query(
    query_embeddings=[question_embedding],
    n_results=4
)
```

---

#### Opci√≥n D: Qdrant

**Ventajas:**
- ‚úÖ Open source
- ‚úÖ Muy r√°pido
- ‚úÖ Buen rendimiento

**Desventajas:**
- ‚ùå Requiere servidor separado
- ‚ùå Menos conocido

---

### 4. Alternativas para Memoria Conversacional

#### Opci√≥n A: Redis (Recomendado para Producci√≥n)

**Ventajas:**
- ‚úÖ Muy r√°pido
- ‚úÖ Persistente
- ‚úÖ Escalable
- ‚úÖ TTL autom√°tico

**Desventajas:**
- ‚ùå Requiere servidor Redis
- ‚ùå Dependencia adicional

**Ejemplo:**
```python
import redis
import json

r = redis.Redis(host='localhost', port=6379, db=0)

# Guardar mensaje
messages = [{"role": "user", "content": "..."}]
r.setex(
    f"chat:{session_id}",
    3600,  # TTL: 1 hora
    json.dumps(messages)
)

# Obtener mensajes
messages = json.loads(r.get(f"chat:{session_id}"))
```

---

#### Opci√≥n B: PostgreSQL (Tabla de Conversaciones)

**Ventajas:**
- ‚úÖ Ya tienes PostgreSQL
- ‚úÖ Persistente
- ‚úÖ Puedes hacer queries complejas

**Desventajas:**
- ‚ùå M√°s lento que Redis
- ‚ùå M√°s complejo de implementar

**Ejemplo:**
```sql
CREATE TABLE chat_messages (
    id UUID PRIMARY KEY,
    session_id VARCHAR(255),
    role VARCHAR(20),  -- 'user' o 'assistant'
    content TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Obtener historial
SELECT role, content
FROM chat_messages
WHERE session_id = 'user123_pet456'
ORDER BY created_at;
```

---

#### Opci√≥n C: MongoDB

**Ventajas:**
- ‚úÖ Flexible (documentos JSON)
- ‚úÖ Bueno para datos no estructurados
- ‚úÖ Escalable

**Desventajas:**
- ‚ùå Requiere servidor MongoDB
- ‚ùå M√°s complejo que Redis

**Ejemplo:**
```python
from pymongo import MongoClient

client = MongoClient('mongodb://localhost:27017/')
db = client['pet_healthcare']
messages = db['chat_messages']

# Guardar mensaje
messages.insert_one({
    "session_id": "user123_pet456",
    "role": "user",
    "content": "Pregunta",
    "timestamp": datetime.now()
})

# Obtener historial
history = list(messages.find(
    {"session_id": "user123_pet456"}
).sort("timestamp", 1))
```

---

## üéØ Enfoques de Implementaci√≥n

### Enfoque 1: RAG con Memoria (Actual)

**Arquitectura:**
```
Pregunta ‚Üí Buscar documentos ‚Üí Construir prompt con historial + documentos ‚Üí LLM ‚Üí Respuesta
```

**Ventajas:**
- ‚úÖ Respuestas basadas en documentos reales
- ‚úÖ Memoria conversacional
- ‚úÖ Preciso para informaci√≥n espec√≠fica

**Desventajas:**
- ‚ùå M√°s lento (b√∫squeda + generaci√≥n)
- ‚ùå M√°s costoso (embeddings + LLM)
- ‚ùå Requiere documentos con texto

**Cu√°ndo usar:**
- Cuando tienes documentos PDF con informaci√≥n espec√≠fica
- Cuando necesitas respuestas precisas basadas en datos reales
- Cuando la informaci√≥n cambia frecuentemente

---

### Enfoque 2: Solo Memoria Conversacional (Sin RAG)

**Arquitectura:**
```
Pregunta ‚Üí Construir prompt con historial ‚Üí LLM ‚Üí Respuesta
```

**Ventajas:**
- ‚úÖ M√°s r√°pido
- ‚úÖ M√°s barato
- ‚úÖ Funciona sin documentos

**Desventajas:**
- ‚ùå No puede acceder a documentos espec√≠ficos
- ‚ùå Respuestas basadas solo en conocimiento general del LLM

**Cu√°ndo usar:**
- Para preguntas generales sobre veterinaria
- Cuando no hay documentos disponibles
- Para consultas r√°pidas

---

### Enfoque 3: RAG sin Memoria

**Arquitectura:**
```
Pregunta ‚Üí Buscar documentos ‚Üí Construir prompt con documentos ‚Üí LLM ‚Üí Respuesta
```

**Ventajas:**
- ‚úÖ Respuestas basadas en documentos
- ‚úÖ M√°s r√°pido que con memoria
- ‚úÖ Menos tokens (no incluye historial)

**Desventajas:**
- ‚ùå No recuerda conversaciones anteriores
- ‚ùå Cada pregunta es independiente

**Cu√°ndo usar:**
- Cuando cada pregunta es independiente
- Cuando no necesitas contexto conversacional
- Para an√°lisis de documentos √∫nicos

---

### Enfoque 4: Memoria con Resumen (Summary Memory)

**Arquitectura:**
```
Pregunta ‚Üí Resumir historial antiguo ‚Üí Construir prompt con resumen + pregunta ‚Üí LLM ‚Üí Respuesta
```

**Ventajas:**
- ‚úÖ Mantiene contexto sin l√≠mite de tokens
- ‚úÖ M√°s eficiente que buffer completo

**Desventajas:**
- ‚ùå Puede perder detalles espec√≠ficos
- ‚ùå M√°s complejo de implementar

**Ejemplo con LangChain:**
```python
from langchain.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(
    llm=llm,  # Necesita LLM para resumir
    return_messages=True
)

# El historial se resume autom√°ticamente cuando crece
```

---

### Enfoque 5: Memoria con Ventana Deslizante

**Arquitectura:**
```
Pregunta ‚Üí Mantener solo √∫ltimos N mensajes ‚Üí Construir prompt ‚Üí LLM ‚Üí Respuesta
```

**Ventajas:**
- ‚úÖ Control preciso sobre tokens
- ‚úÖ Mantiene contexto reciente
- ‚úÖ Simple de implementar

**Desventajas:**
- ‚ùå Pierde contexto antiguo
- ‚ùå Puede perder informaci√≥n importante

**Implementaci√≥n actual:**
```python
# Ya implementado en ChatController._limit_memory_messages()
# Mantiene solo los √∫ltimos 12 mensajes (6 interacciones)
```

---

### Enfoque 6: Memoria con Base de Conocimiento Externa

**Arquitectura:**
```
Pregunta ‚Üí Buscar en KB ‚Üí Construir prompt con KB + historial ‚Üí LLM ‚Üí Respuesta
```

**Ventajas:**
- ‚úÖ Puede acceder a informaci√≥n estructurada
- ‚úÖ M√°s preciso que solo documentos PDF
- ‚úÖ Puede combinar m√∫ltiples fuentes

**Desventajas:**
- ‚ùå M√°s complejo
- ‚ùå Requiere mantener base de conocimiento

**Ejemplo:**
```python
# Buscar en base de conocimiento estructurada
kb_results = knowledge_base.search(question)

# Combinar con documentos PDF
all_context = kb_results + pdf_documents

# Generar respuesta
response = llm.invoke(prompt_with_context)
```

---

## üíæ Memoria Conversacional

### Tipos de Memoria en LangChain

#### 1. ConversationBufferMemory (Actual)

**C√≥mo funciona:**
- Almacena todos los mensajes en orden
- No tiene l√≠mite por defecto
- Simple y directo

**Implementaci√≥n:**
```python
memory = ConversationBufferMemory(
    return_messages=True,  # Retorna lista de mensajes
    memory_key="chat_history",
    output_key="answer"
)

# Guardar
memory.save_context(
    {"input": "Pregunta"},
    {"output": "Respuesta"}
)

# Obtener
memory_vars = memory.load_memory_variables({})
history = memory_vars["chat_history"]  # Lista de mensajes
```

**Estructura interna:**
```python
memory.chat_memory.messages = [
    HumanMessage(content="Pregunta 1"),
    AIMessage(content="Respuesta 1"),
    HumanMessage(content="Pregunta 2"),
    AIMessage(content="Respuesta 2"),
    ...
]
```

---

#### 2. ConversationSummaryMemory

**C√≥mo funciona:**
- Resume el historial antiguo cuando crece
- Mantiene mensajes recientes completos
- Usa LLM para generar resumen

**Ventajas:**
- ‚úÖ Mantiene contexto sin l√≠mite de tokens
- ‚úÖ Eficiente para conversaciones largas

**Desventajas:**
- ‚ùå Puede perder detalles en el resumen
- ‚ùå Requiere llamadas adicionales al LLM

**Ejemplo:**
```python
from langchain.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(
    llm=llm,  # Necesita LLM para resumir
    return_messages=True
)

# Cuando el historial crece:
# - Mensajes antiguos ‚Üí Se resumen
# - Mensajes recientes ‚Üí Se mantienen completos
```

---

#### 3. ConversationBufferWindowMemory

**C√≥mo funciona:**
- Mantiene solo los √∫ltimos N mensajes
- Similar a lo que ya implementamos manualmente

**Ejemplo:**
```python
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(
    k=6,  # Mantiene √∫ltimos 6 mensajes
    return_messages=True
)
```

---

#### 4. ConversationSummaryBufferMemory

**C√≥mo funciona:**
- Combina buffer y summary
- Mantiene √∫ltimos N mensajes completos
- Resume el resto

**Ejemplo:**
```python
from langchain.memory import ConversationSummaryBufferMemory

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=2000,  # L√≠mite de tokens
    return_messages=True
)
```

---

### Implementaci√≥n de Memoria en el Sistema Actual

**Almacenamiento:**
```python
# En memoria (no persistente)
_conversation_memories: Dict[str, ConversationBufferMemory] = {}

# Estructura:
# {
#   "user123_pet456": ConversationBufferMemory(...),
#   "user789_pet101": ConversationBufferMemory(...),
#   ...
# }
```

**Limitaci√≥n Manual:**
```python
def _limit_memory_messages(memory):
    # Obtiene todos los mensajes
    messages = memory.load_memory_variables({})["chat_history"]
    
    # Si excede 12 mensajes (6 interacciones)
    if len(messages) > 12:
        # Mantiene solo los √∫ltimos 12
        messages_to_keep = messages[-12:]
        
        # Limpia y restaura
        memory.chat_memory.clear()
        for msg in messages_to_keep:
            memory.chat_memory.add_message(msg)
```

**¬øPor qu√© limitar a 6 interacciones?**
- Control de costos (menos tokens = menos costo)
- Evitar que el contexto sea demasiado largo
- Mejor rendimiento (menos procesamiento)

---

## üîç RAG (Retrieval Augmented Generation)

### ¬øQu√© es RAG?

**RAG** es una t√©cnica que combina:
1. **Retrieval (Recuperaci√≥n)**: Buscar informaci√≥n relevante en documentos
2. **Augmented (Aumentado)**: Aumentar el prompt con esa informaci√≥n
3. **Generation (Generaci√≥n)**: Generar respuesta usando el contexto

### Flujo de RAG

```
1. Usuario pregunta: "¬øCu√°ndo fue la √∫ltima vacunaci√≥n?"
                    ‚Üì
2. Generar embedding de la pregunta
   [0.123, -0.456, ..., 0.789]  (1536 dimensiones)
                    ‚Üì
3. Buscar en vector store (b√∫squeda por similitud)
   - Comparar embedding de pregunta con embeddings de chunks
   - Encontrar top 4 chunks m√°s similares
                    ‚Üì
4. Recuperar chunks relevantes
   - "Vacunaci√≥n aplicada el 17/01/2019..."
   - "Pr√≥xima dosis: 18/01/2019..."
                    ‚Üì
5. Construir prompt aumentado
   System: "Eres un veterinario experto..."
   Context: "Documentos relevantes: [chunks recuperados]"
   History: "[conversaci√≥n anterior]"
   Question: "¬øCu√°ndo fue la √∫ltima vacunaci√≥n?"
                    ‚Üì
6. Enviar a LLM
                    ‚Üì
7. LLM genera respuesta usando el contexto
   "Seg√∫n los documentos, la √∫ltima vacunaci√≥n fue el 17/01/2019..."
                    ‚Üì
8. Retornar respuesta + documentos fuente
```

### Componentes de RAG

#### 1. Document Loader

**Prop√≥sito:** Cargar documentos desde diferentes fuentes

**Opciones:**
- `PyPDFLoader`: PDFs locales o URLs
- `UnstructuredFileLoader`: M√∫ltiples formatos
- `CSVLoader`: Archivos CSV
- `TextLoader`: Archivos de texto plano

**Ejemplo:**
```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("documento.pdf")
documents = loader.load()  # Lista de Document (una por p√°gina)
```

---

#### 2. Text Splitter

**Prop√≥sito:** Dividir documentos largos en chunks m√°s peque√±os

**Tipos:**
- `RecursiveCharacterTextSplitter`: Divide por caracteres (actual)
- `TokenTextSplitter`: Divide por tokens
- `CharacterTextSplitter`: Divide por caracteres (simple)
- `MarkdownHeaderTextSplitter`: Divide markdown por headers

**Par√°metros importantes:**
```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Tama√±o m√°ximo del chunk
    chunk_overlap=200,    # Overlap entre chunks
    length_function=len   # Funci√≥n para medir longitud
)
```

**¬øPor qu√© overlap?**
- Evita perder informaci√≥n en los bordes
- Mejora la recuperaci√≥n de informaci√≥n que cruza chunks

---

#### 3. Embeddings

**Prop√≥sito:** Convertir texto a vectores num√©ricos

**Modelos disponibles:**
- `OpenAIEmbeddings`: text-embedding-3-small, text-embedding-3-large
- `HuggingFaceEmbeddings`: Modelos open source
- `CohereEmbeddings`: API de Cohere
- `SentenceTransformers`: Modelos locales

**Ejemplo:**
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Generar embedding
vector = embeddings.embed_query("Vacunaci√≥n aplicada el 17/01/2019")
# Resultado: [0.123, -0.456, ..., 0.789]  (1536 n√∫meros)
```

**Dimensiones:**
- `text-embedding-3-small`: 1536 dimensiones
- `text-embedding-3-large`: 3072 dimensiones (m√°s preciso, m√°s caro)

---

#### 4. Vector Store

**Prop√≥sito:** Almacenar y buscar vectores eficientemente

**Opciones:**
- `PGVector`: PostgreSQL + pgvector (actual)
- `Pinecone`: Servicio gestionado
- `Weaviate`: Open source
- `Chroma`: Embeddable
- `FAISS`: Local, muy r√°pido
- `Qdrant`: Open source, r√°pido

**B√∫squeda por similitud:**
```python
# B√∫squeda por distancia coseno (similaridad)
results = vector_store.similarity_search(
    query="vacunaci√≥n",
    k=4  # Top 4 resultados
)

# B√∫squeda con score (distancia)
results = vector_store.similarity_search_with_score(
    query="vacunaci√≥n",
    k=4
)
# Retorna: [(Document, score), ...]
```

---

#### 5. Retriever

**Prop√≥sito:** Interfaz para recuperar documentos relevantes

**Tipos:**
- `VectorStoreRetriever`: B√∫squeda por similitud (actual)
- `BM25Retriever`: B√∫squeda por palabras clave
- `EnsembleRetriever`: Combina m√∫ltiples m√©todos
- `ContextualCompressionRetriever`: Comprime contexto

**Ejemplo:**
```python
retriever = vector_store.as_retriever(
    search_kwargs={"k": 4}  # Top 4 documentos
)

# Buscar
docs = retriever.get_relevant_documents("vacunaci√≥n")
```

---

#### 6. Chain

**Prop√≥sito:** Orquestar todo el flujo RAG

**Tipos:**
- `ConversationalRetrievalChain`: RAG + memoria (actual)
- `RetrievalQA`: RAG simple sin memoria
- `RetrievalQAWithSourcesChain`: RAG con fuentes

**Flujo interno:**
```python
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True
)

# Internamente hace:
# 1. Obtiene historial de memoria
# 2. Busca documentos relevantes
# 3. Construye prompt con historial + documentos + pregunta
# 4. Invoca LLM
# 5. Guarda en memoria
# 6. Retorna respuesta + documentos fuente
```

---

### Optimizaciones de RAG

#### 1. Chunking Inteligente

**Problema:** Dividir por caracteres puede cortar informaci√≥n importante

**Soluci√≥n:** Dividir por estructura sem√°ntica

```python
from langchain.text_splitters import MarkdownHeaderTextSplitter

# Dividir markdown por headers
splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[("#", "Header 1")])
chunks = splitter.split_text(markdown_text)
```

---

#### 2. Re-ranking

**Problema:** Los primeros resultados pueden no ser los m√°s relevantes

**Soluci√≥n:** Re-ordenar resultados con modelo de re-ranking

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# Comprimir y re-ordenar
compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)
```

---

#### 3. Hybrid Search

**Problema:** B√∫squeda solo por similitud puede perder informaci√≥n

**Soluci√≥n:** Combinar b√∫squeda sem√°ntica + b√∫squeda por palabras clave

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# B√∫squeda h√≠brida
bm25_retriever = BM25Retriever.from_documents(documents)
vector_retriever = vector_store.as_retriever()

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.5, 0.5]  # 50% cada uno
)
```

---

#### 4. Metadata Filtering

**Problema:** Buscar en todos los documentos puede ser ineficiente

**Soluci√≥n:** Filtrar por metadata antes de buscar

```python
# Filtrar por metadata
retriever = vector_store.as_retriever(
    search_kwargs={
        "k": 4,
        "filter": {"pet_id": "876835fa-6c7d-4c97-bc18-4e5728e8bc13"}
    }
)
```

---

## ‚ö° Optimizaciones y Mejores Pr√°cticas

### 1. Optimizaci√≥n de Costos

#### Reducir Tokens

```python
# ‚ùå Mal: Incluir todo el historial
prompt = f"{full_history}\n{question}"

# ‚úÖ Bien: Limitar historial
recent_history = history[-6:]  # Solo √∫ltimos 6 mensajes
prompt = f"{recent_history}\n{question}"

# ‚úÖ Mejor: Resumir historial antiguo
summary = summarize_old_history(history[:-6])
prompt = f"{summary}\n{recent_history}\n{question}"
```

#### Usar Modelos M√°s Baratos

```python
# Para embeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")  # M√°s barato

# Para generaci√≥n
llm = ChatOpenAI(model="gpt-4o-mini")  # M√°s barato que gpt-4o
```

#### Cachear Embeddings

```python
# No regenerar embeddings de documentos que no han cambiado
# Guardar embeddings en base de datos con hash del documento
```

---

### 2. Optimizaci√≥n de Rendimiento

#### Pre-cargar Vector Store

```python
# En lugar de crear vector store en cada pregunta
# Crear una vez y reutilizar
vector_store_cache = {}

def get_vector_store(pet_id):
    if pet_id not in vector_store_cache:
        vector_store_cache[pet_id] = create_vector_store(pet_id)
    return vector_store_cache[pet_id]
```

#### Usar Streaming

```python
# Respuestas en tiempo real (token por token)
for chunk in chain.stream({"question": question}):
    print(chunk, end="", flush=True)
```

#### Paralelizar B√∫squedas

```python
# Buscar en m√∫ltiples colecciones en paralelo
import asyncio

async def search_multiple_sources(question):
    results = await asyncio.gather(
        search_vaccinations(question),
        search_visits(question),
        search_lab_results(question)
    )
    return combine_results(results)
```

---

### 3. Mejora de Calidad

#### Mejorar Prompts

```python
# ‚ùå Mal: Prompt gen√©rico
prompt = "Responde la pregunta: {question}"

# ‚úÖ Bien: Prompt espec√≠fico con instrucciones claras
prompt = """Eres un veterinario experto. 
Responde SOLO bas√°ndote en los documentos proporcionados.
Si la informaci√≥n no est√° en los documentos, di claramente que no la encontraste.
S√© espec√≠fico y menciona fechas cuando est√©n disponibles.

Documentos: {context}
Pregunta: {question}
Respuesta:"""
```

#### Validar Respuestas

```python
# Validar que la respuesta es relevante
def validate_answer(answer, question, documents):
    # Verificar que menciona informaci√≥n de los documentos
    # Verificar que responde la pregunta
    # Verificar que no alucina informaci√≥n
    pass
```

#### Usar Few-Shot Examples

```python
prompt = """Eres un veterinario experto.

Ejemplos:
Usuario: "¬øCu√°ndo fue la √∫ltima vacunaci√≥n?"
Veterinario: "Seg√∫n los documentos, la √∫ltima vacunaci√≥n fue el 17/01/2019 con la vacuna Vanguard Plus 5 L4."

Usuario: {question}
Veterinario:"""
```

---

### 4. Manejo de Errores

#### Timeouts

```python
import asyncio

try:
    response = await asyncio.wait_for(
        llm.ainvoke(messages),
        timeout=30.0  # 30 segundos m√°ximo
    )
except asyncio.TimeoutError:
    return {"error": "Timeout al generar respuesta"}
```

#### Retry Logic

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def ask_question_with_retry(question):
    return llm.invoke(question)
```

#### Fallbacks

```python
try:
    # Intentar con documentos
    answer = ask_with_rag(question, documents)
except Exception:
    # Fallback a conversaci√≥n general
    answer = ask_without_documents(question)
```

---

## üìä Comparaci√≥n de Enfoques

### Tabla Comparativa

| Enfoque | Velocidad | Costo | Precisi√≥n | Complejidad | Memoria |
|---------|----------|------|-----------|-------------|---------|
| RAG + Memoria (Actual) | Media | Alto | Alta | Alta | ‚úÖ |
| Solo Memoria | Alta | Media | Media | Baja | ‚úÖ |
| RAG sin Memoria | Media | Alto | Alta | Media | ‚ùå |
| Memoria con Resumen | Media | Media | Media | Alta | ‚úÖ |
| Directo OpenAI | Alta | Bajo | Baja | Baja | ‚ùå |

---

## üéì Conceptos Clave para Entender

### 1. Embeddings

**¬øQu√© son?**
- Representaci√≥n num√©rica del significado del texto
- Textos similares tienen vectores similares
- Permite b√∫squeda sem√°ntica (no solo palabras clave)

**Ejemplo:**
```
"Vacunaci√≥n de perro" ‚Üí [0.1, -0.3, 0.5, ...]
"Vacuna canina"      ‚Üí [0.12, -0.28, 0.52, ...]  (similar)
"Comida de gato"     ‚Üí [-0.2, 0.4, -0.1, ...]   (diferente)
```

**Distancia:**
- **Coseno**: Mide el √°ngulo entre vectores (0-1, donde 1 = id√©ntico)
- **Euclidiana**: Mide distancia directa
- **Punto**: Producto punto (m√°s r√°pido)

---

### 2. Vector Store

**¬øQu√© es?**
- Base de datos especializada en almacenar y buscar vectores
- Permite b√∫squeda por similitud muy r√°pida

**√çndices:**
- **IVFFlat**: √çndice invertido (r√°pido, aproximado)
- **HNSW**: Hierarchical Navigable Small World (muy r√°pido)
- **Exact**: B√∫squeda exacta (lento, preciso)

---

### 3. Chunking

**¬øPor qu√© dividir?**
- LLMs tienen l√≠mite de tokens (ej: 128K para GPT-4)
- B√∫squeda m√°s precisa (chunks peque√±os = m√°s espec√≠ficos)
- Mejor rendimiento (menos tokens a procesar)

**Estrategias:**
- **Por tama√±o**: Dividir en chunks de N caracteres
- **Por estructura**: Dividir por p√°rrafos, headers, etc.
- **Por sem√°ntica**: Dividir por significado (m√°s complejo)

---

### 4. Retrieval

**M√©todos:**
- **Dense Retrieval**: B√∫squeda por embeddings (sem√°ntica)
- **Sparse Retrieval**: B√∫squeda por palabras clave (BM25, TF-IDF)
- **Hybrid**: Combinaci√≥n de ambos

**Re-ranking:**
- Re-ordenar resultados con modelo m√°s sofisticado
- Mejora precisi√≥n pero aumenta costo

---

### 5. Generation

**Par√°metros importantes:**
- **Temperature**: Creatividad (0.0 = determinista, 1.0 = creativo)
- **Max Tokens**: Longitud m√°xima de respuesta
- **Top P**: Nucleus sampling (controla diversidad)
- **Frequency Penalty**: Penaliza repeticiones

---

## üî¨ Ejemplo de Implementaci√≥n Completa

### Versi√≥n Simplificada (Sin Frameworks)

```python
import openai
import psycopg
from pgvector.psycopg import register_vector
import json

class SimpleVetChat:
    def __init__(self, api_key, db_url):
        self.api_key = api_key
        self.db_url = db_url
        self.memories = {}  # {session_id: [mensajes]}
    
    def ask_question(self, question, session_id, pet_id=None):
        # 1. Obtener historial
        history = self.memories.get(session_id, [])
        
        # 2. Buscar documentos si hay pet_id
        context = ""
        if pet_id:
            context = self._search_documents(question, pet_id)
        
        # 3. Construir mensajes
        messages = [
            {"role": "system", "content": "Eres un veterinario experto..."}
        ]
        
        # Agregar historial
        for msg in history[-6:]:  # √öltimos 6 mensajes
            messages.append(msg)
        
        # Agregar contexto si hay documentos
        if context:
            messages.append({
                "role": "system",
                "content": f"Documentos relevantes:\n{context}"
            })
        
        # Agregar pregunta actual
        messages.append({"role": "user", "content": question})
        
        # 4. Llamar a OpenAI
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.3
        )
        
        answer = response.choices[0].message.content
        
        # 5. Guardar en memoria
        history.append({"role": "user", "content": question})
        history.append({"role": "assistant", "content": answer})
        
        # Limitar a 12 mensajes (6 interacciones)
        if len(history) > 12:
            history = history[-12:]
        
        self.memories[session_id] = history
        
        return {
            "answer": answer,
            "chat_history": history
        }
    
    def _search_documents(self, question, pet_id):
        # 1. Generar embedding de pregunta
        embedding_response = openai.embeddings.create(
            model="text-embedding-3-small",
            input=question
        )
        question_embedding = embedding_response.data[0].embedding
        
        # 2. Buscar en PostgreSQL
        conn = psycopg.connect(self.db_url)
        register_vector(conn)
        
        with conn.cursor() as cur:
            cur.execute("""
                SELECT document->>'page_content' as content
                FROM langchain_pg_embedding
                WHERE collection_id = (
                    SELECT uuid FROM langchain_pg_collection 
                    WHERE name = %s
                )
                ORDER BY embedding <-> %s::vector
                LIMIT 4
            """, (f"pet_{pet_id}_documents", question_embedding))
            
            results = cur.fetchall()
        
        # 3. Combinar resultados
        context = "\n\n".join([r[0] for r in results])
        return context
```

---

## üöÄ Mejoras Futuras

### 1. Persistencia de Memoria

**Actual:** Memoria en RAM (se pierde al reiniciar)

**Mejora:** Usar Redis o PostgreSQL

```python
# Con Redis
import redis
import json

r = redis.Redis(...)

def save_memory(session_id, messages):
    r.setex(
        f"chat:{session_id}",
        3600,  # TTL: 1 hora
        json.dumps(messages)
    )

def load_memory(session_id):
    data = r.get(f"chat:{session_id}")
    return json.loads(data) if data else []
```

---

### 2. Streaming de Respuestas

**Actual:** Respuesta completa al final

**Mejora:** Respuesta token por token

```python
from fastapi.responses import StreamingResponse

@router.post("/chat/pets/{pet_id}/ask-stream")
async def ask_streaming(pet_id: str, request: ChatQuestionRequest):
    async def generate():
        async for chunk in chain.astream({"question": request.question}):
            yield f"data: {json.dumps(chunk)}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

---

### 3. Cach√© de Respuestas

**Mejora:** Cachear respuestas a preguntas comunes

```python
import hashlib
import redis

def get_cached_answer(question):
    question_hash = hashlib.md5(question.encode()).hexdigest()
    cached = r.get(f"answer:{question_hash}")
    return json.loads(cached) if cached else None

def cache_answer(question, answer):
    question_hash = hashlib.md5(question.encode()).hexdigest()
    r.setex(f"answer:{question_hash}", 3600, json.dumps(answer))
```

---

### 4. An√°lisis de Sentimiento

**Mejora:** Detectar urgencia en preguntas

```python
from transformers import pipeline

sentiment_analyzer = pipeline("sentiment-analysis")

def analyze_urgency(question):
    result = sentiment_analyzer(question)
    if "urgente" in question.lower() or result[0]["label"] == "NEGATIVE":
        return "high"
    return "normal"
```

---

## üìö Recursos Adicionales

### Documentaci√≥n Oficial
- **LangChain**: https://python.langchain.com/
- **OpenAI API**: https://platform.openai.com/docs
- **pgvector**: https://github.com/pgvector/pgvector
- **FastAPI**: https://fastapi.tiangolo.com/

### Tutoriales Recomendados
- **RAG Tutorial**: https://python.langchain.com/docs/use_cases/question_answering/
- **Memory Tutorial**: https://python.langchain.com/docs/modules/memory/
- **Vector Stores**: https://python.langchain.com/docs/integrations/vectorstores/

### Herramientas √ötiles
- **LangSmith**: Monitoreo y debugging de aplicaciones LangChain
- **Weights & Biases**: Tracking de experimentos
- **Postman**: Probar endpoints de la API

---

## üéØ Resumen Ejecutivo

### Arquitectura Actual

1. **Cliente** ‚Üí FastAPI ‚Üí **Controlador** ‚Üí **Servicio LangChain**
2. **Servicio** carga PDFs, genera embeddings, almacena en PGVector
3. **Pregunta** ‚Üí Busca documentos relevantes ‚Üí Construye prompt ‚Üí LLM ‚Üí Respuesta
4. **Memoria** se mantiene en RAM (limitada a 6 interacciones)

### Tecnolog√≠as Clave

- **LangChain**: Framework para orquestar LLMs
- **OpenAI**: GPT-4o-mini para generaci√≥n, text-embedding-3-small para embeddings
- **pgvector**: Almacenamiento de vectores en PostgreSQL
- **ConversationBufferMemory**: Memoria conversacional

### Alternativas Principales

- **LlamaIndex**: Mejor para RAG puro
- **Directo OpenAI**: M√°s control, menos abstracci√≥n
- **Redis**: Mejor para memoria en producci√≥n
- **Pinecone/Weaviate**: Vector stores gestionados

---

**√öltima actualizaci√≥n:** Enero 2025  
**Versi√≥n:** 1.0


